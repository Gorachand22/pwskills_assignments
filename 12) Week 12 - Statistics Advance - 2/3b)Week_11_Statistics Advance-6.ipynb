{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHKORN_4cJKm"
      },
      "source": [
        "<h1 style = 'color:red'><b>Week-12, Statistics Advance-6 Assignment</b><h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUW29xlcJKo"
      },
      "source": [
        "Name - Gorachanda Dash <br>\n",
        "Date - 13-Mar-2023<br>\n",
        "Week 12, Statistics Advance-6 Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKnfvSL5cJK2"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analysis of Variance (ANOVA) is a statistical technique used to compare means across multiple groups and determine if there are statistically significant differences among them. ANOVA makes certain assumptions about the data in order to provide reliable results. Violations of these assumptions can impact the validity of ANOVA results. The main assumptions of ANOVA are:\n",
        "\n",
        "1. **`Normality`**: The dependent variable should follow a normal distribution within each group. This assumption is important because ANOVA relies on the normal distribution for its calculations. Violations can lead to inaccurate p-values and confidence intervals.\n",
        "\n",
        "2. **`Homogeneity of Variance (Homoscedasticity)`**: The variances of the dependent variable should be approximately equal across all groups. Homogeneity of variance is crucial for the F-test in ANOVA to be valid. Violations can result in an inflated Type I error rate (false positives) or Type II error rate (false negatives).\n",
        "\n",
        "3. **`Independence`**: Observations within each group should be independent of each other. This means that the value of an observation in one group should not be influenced by the value of an observation in another group. Violations can lead to biased estimates of treatment effects and inaccurate p-values.\n",
        "\n",
        "`Examples of Violations and Their Impact:`\n",
        "\n",
        "1. **`Normality Violation`**: If the data within each group is not normally distributed, the F-test results may be inaccurate. For instance, if one group's data is heavily skewed or has extreme outliers, the F-test may incorrectly identify significant differences between groups.\n",
        "\n",
        "2. **`Homoscedasticity Violation`**: If the variances of the groups are not equal, the F-test results can be compromised. If one group has larger variance than the others, the F-test may incorrectly identify that group as significantly different even if the means are not truly different.\n",
        "\n",
        "3. **`Independence Violation`**: If observations are not independent, it can lead to inflated Type I error rates. For example, in a repeated measures design where each subject is measured at multiple time points, the assumption of independence is violated. If this assumption is ignored, it may lead to false conclusions about group differences.\n",
        "\n",
        "4. **`Outliers`**: Outliers can affect both normality and homoscedasticity assumptions. They can distort the distribution and influence the calculation of means and variances. Outliers may lead to the detection of significant differences that do not actually exist or mask true differences.\n",
        "\n",
        "5. **`Unequal Sample Sizes`**: While ANOVA is robust to slightly unequal sample sizes, highly unequal sizes can affect the validity of results. The group with the larger sample size tends to have more influence on the overall results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nokEV7cJK3"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q2. What are the three types of ANOVA, and in what situations would each be used?</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are three main types of Analysis of Variance (ANOVA) techniques, each designed for different experimental setups and research questions. These three types of ANOVA are:\n",
        "\n",
        "1. **`One-Way ANOVA`**:\n",
        "   - **`Usage`**: One-Way ANOVA is used when you have one categorical independent variable (factor) with more than two levels (groups) and a continuous dependent variable.\n",
        "   - *`*Example`**: Suppose you want to test whether the mean scores on a test differ among three different teaching methods (e.g., traditional lecture, online tutorial, group discussion). The teaching method is the independent variable, and the test scores are the dependent variable.\n",
        "\n",
        "2. **`Two-Way ANOVA`**:\n",
        "   - **`Usage`**: Two-Way ANOVA is used when you have two categorical independent variables (factors) and a continuous dependent variable. It allows you to analyze the main effects of each factor and their interaction.\n",
        "   - **`Example`**: Consider a study where you want to investigate the effects of both teaching method (online vs. in-person) and study environment (quiet vs. noisy) on exam scores. Here, you have two independent variables (teaching method and study environment).\n",
        "\n",
        "3. **`Repeated Measures ANOVA`**:\n",
        "   - **`Usage`**: Repeated Measures ANOVA is used when you have a single group of participants and you measure the same variable under different conditions or at multiple time points.\n",
        "   - **`Example`**: Suppose you're studying the effects of a new drug on blood pressure, and you measure the participants' blood pressure before taking the drug, right after taking it, and then 30 minutes later. Here, the repeated measures factor is the time of measurement.\n",
        "\n",
        "These types of ANOVA are chosen based on the research design and the number of factors you are investigating. Each type of ANOVA has its own set of assumptions, and the choice depends on the complexity of your experimental design.\n",
        "\n",
        "- **`One-Way ANOVA`** is appropriate when you have only one categorical factor with multiple levels and want to compare their means.\n",
        "- **`Two-Way ANOVA`** is suitable when you have two categorical factors and want to assess their main effects and interaction effect on a continuous outcome.\n",
        "- **`Repeated Measures ANOVA`** is useful when you have multiple measurements on the same individuals under different conditions or time points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMuh9StycJK3"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partitioning of variance in ANOVA refers to the process of decomposing the total variability in a dataset into different components that can be attributed to specific sources. Understanding this concept is crucial because it helps us analyze and understand the sources of variability in the data, identify the contributions of different factors, and determine if the observed differences are statistically significant.\n",
        "\n",
        "In ANOVA, the total variability in the dependent variable is divided into two main components:\n",
        "\n",
        "1. **`Between-Group Variability (Treatment Variability)`**: This component represents the variability in the data that can be attributed to the differences between the groups or levels of the independent variable (factor). It measures how much the group means deviate from the overall mean. If the between-group variability is large relative to the within-group variability, it suggests that the independent variable has a significant effect on the dependent variable.\n",
        "\n",
        "2. **`Within-Group Variability (Error Variability)`**: This component represents the variability that cannot be explained by the differences between the groups. It includes the inherent variability within each group, measurement errors, and any other uncontrolled factors. It serves as a baseline measure of variability. Smaller within-group variability indicates that the observations within each group are relatively consistent.\n",
        "\n",
        "The importance of understanding the partitioning of variance in ANOVA includes:\n",
        "\n",
        "1. **`Identifying Sources of Variation`**: By decomposing the total variability, ANOVA allows us to determine which sources of variation are contributing to the observed differences in the data. This is crucial for understanding the factors that might be influencing the dependent variable.\n",
        "\n",
        "2. **`Assessing Treatment Effects`**: ANOVA helps us determine whether the observed differences between group means are statistically significant or if they could have occurred due to random chance. By comparing the between-group variability to the within-group variability, ANOVA helps us assess the significance of the treatment effect.\n",
        "\n",
        "3. **`Interpreting Results`**: The partitioning of variance provides insights into the relative importance of the factors being studied. It helps researchers interpret the results and draw meaningful conclusions about the relationships between variables.\n",
        "\n",
        "4. **`Optimizing Experimental Designs`**: Understanding how much variability is due to treatment effects and how much is due to error helps researchers design experiments more effectively. Minimizing error variability increases the power of the analysis to detect true treatment effects.\n",
        "\n",
        "5. **`Basis for F-Test`**: The F-statistic in ANOVA is calculated by comparing the ratio of between-group variability to within-group variability. This F-test is used to determine whether the differences between group means are statistically significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aKHN4uEcJK4"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Certainly, let's calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) manually for a one-way ANOVA example:\n",
        "\n",
        "`Given data (grouped data):`<br>\n",
        "```python\n",
        "Group 1: [12, 14, 15, 16, 18]\n",
        "Group 2: [20, 22, 23, 24, 25]\n",
        "Group 3: [30, 31, 32, 34, 35]\n",
        "```\n",
        "\n",
        "`Step 1: Calculate the Total Mean`<br>\n",
        "`Calculate the mean of all data points combined:`<br>\n",
        "Total Mean = ‚àëall data points / total number of data points = {12 + 14 +....+ 35}/{15} ‚âà 24.133\n",
        "\n",
        "`Step 2: Calculate the Total Sum of Squares (SST)`<br>\n",
        "Calculate the squared differences between each data point and the total mean, then sum them up:<br>\n",
        "SST = (12 - 24.133)^2 + (14 - 24.133)^2 + ..... + (35 - 24.133)^2 ‚âà 811.6\n",
        "\n",
        "`Step 3: Calculate the Group Means`<br>\n",
        "Calculate the mean for each group:<br>\n",
        "Group 1 Mean = (12 + 14 + 15 + 16 + 18) / 5 = 15\n",
        "Group 2 Mean = (20 + 22 + 23 + 24 + 25) / 5 = 22.8 \n",
        "Group 3 Mean = (30 + 31 + 32 + 34 + 35) / 5 = 32.4\n",
        "\n",
        "`Step 4: Calculate the Explained Sum of Squares (SSE)`<br>\n",
        "Calculate the squared differences between each group mean and the total mean, weighted by the number of observations in each group, then sum them up:\n",
        "SSE = 5 x (15 - 24.133)^2 + 5 x (22.8 - 24.133)^2 + 5 x (32.4 - 24.133)^2 ‚âà 759.59\n",
        "\n",
        "`Step 5: Calculate the Residual Sum of Squares (SSR)`<br>\n",
        "SSR = SST - SSE ‚âà 52\n",
        "\n",
        "In summary: <br>\n",
        "- Total Sum of Squares (SST) ‚âà 811.6\n",
        "- Explained Sum of Squares (SSE) ‚âà 759.59\n",
        "- Residual Sum of Squares (SSR) ‚âà 52\n",
        "\n",
        "These values provide insights into the variation within and between groups in a one-way ANOVA. Remember, the goal is to determine if the explained variation (SSE) is significantly larger than the residual variation (SSR), which would suggest that the group means are not equal and there are statistically significant differences between the groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Sum of Squares (SST): 811.6\n",
            "Explained Sum of Squares (SSE): 759.5999999999999\n",
            "Residual Sum of Squares (SSR): 52.000000000000114\n",
            "F-Statistic: 87.64615384615364\n",
            "P-Value: 6.917797290562078e-08\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample data for each group (replace with your actual data)\n",
        "group1 = np.array([12, 14, 15, 16, 18])\n",
        "group2 = np.array([20, 22, 23, 24, 25])\n",
        "group3 = np.array([30, 31, 32, 34, 35])\n",
        "\n",
        "# Combine all data into one array\n",
        "all_data = np.concatenate([group1, group2, group3])\n",
        "\n",
        "# Calculate the total mean\n",
        "total_mean = np.mean(all_data)\n",
        "\n",
        "# Calculate the Total Sum of Squares (SST)\n",
        "sst = np.sum((all_data - total_mean)**2)\n",
        "\n",
        "# Calculate the Group Means\n",
        "group_means = [np.mean(group1), np.mean(group2), np.mean(group3)]\n",
        "\n",
        "# Calculate the Explained Sum of Squares (SSE)\n",
        "sse = np.sum([(group_mean - total_mean)**2 * len(group) for group_mean, group in zip(group_means, [group1, group2, group3])])\n",
        "\n",
        "# Calculate the Residual Sum of Squares (SSR)\n",
        "ssr = sst - sse\n",
        "\n",
        "# Degrees of freedom\n",
        "df_between = len(group_means) - 1\n",
        "df_within = len(all_data) - len(group_means)\n",
        "\n",
        "# Calculate Mean Square (MS) for Between and Within\n",
        "ms_between = sse / df_between\n",
        "ms_within = ssr / df_within\n",
        "\n",
        "# Calculate F-statistic\n",
        "f_statistic = ms_between / ms_within\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = 1 - stats.f.cdf(f_statistic, df_between, df_within)\n",
        "\n",
        "print(\"Total Sum of Squares (SST):\", sst)\n",
        "print(\"Explained Sum of Squares (SSE):\", sse)\n",
        "print(\"Residual Sum of Squares (SSR):\", ssr)\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-Value:\", p_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2avHLHDcJK4"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANOVA Table:\n",
            "                            sum_sq    df             F    PR(>F)\n",
            "C(Group1)            9.171135e-10   9.0  4.114713e-13  0.999999\n",
            "C(Group2)            1.902448e-10   9.0  8.535505e-14  1.000000\n",
            "C(Group1):C(Group2)  2.071863e+04  81.0  1.032844e+00  0.506108\n",
            "Residual             2.724167e+03  11.0           NaN       NaN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\babul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:1888: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 9, but rank is 1\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "c:\\Users\\babul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:1888: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 9, but rank is 1\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "c:\\Users\\babul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:1888: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 81, but rank is 31\n",
            "  warnings.warn('covariance of constraints does not have full '\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Create a sample dataset\n",
        "data = {\n",
        "    'Group1': np.random.randint(1, 11, 50),\n",
        "    'Group2': np.random.randint(1, 11, 50),\n",
        "    'Values': np.random.randint(50, 101, 50)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Fit the two-way ANOVA model\n",
        "model = ols('Values ~ C(Group1) + C(Group2) + C(Group1):C(Group2)', data=df).fit()\n",
        "\n",
        "# Perform the analysis of variance\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(\"ANOVA Table:\\n\", anova_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When interpreting the results of a one-way ANOVA, the F-statistic and the associated p-value play a crucial role in determining whether there are statistically significant differences between the groups. Let's analyze the situation based on the given values:\n",
        "\n",
        "- F-statistic: 5.23\n",
        "- p-value: 0.02\n",
        "\n",
        "`Interpretation:`\n",
        "\n",
        "1. **`F-Statistic`**: The F-statistic represents the ratio of the variability between the group means to the variability within the groups. In this case, the F-statistic is 5.23.\n",
        "\n",
        "2. **`p-Value`**: The p-value associated with the F-statistic measures the probability of observing such extreme or more extreme data if the null hypothesis (no significant differences between group means) is true. A low p-value suggests that the observed differences between the groups are unlikely to occur due to random chance.\n",
        "\n",
        "`Given the information provided:`\n",
        "\n",
        "- The p-value is 0.02, which is less than the commonly used significance level of 0.05. This indicates that the p-value is statistically significant.\n",
        "\n",
        "`Based on these results:`\n",
        "\n",
        "- Since the p-value is less than 0.05 (commonly used significance level), we reject the null hypothesis.\n",
        "\n",
        "``Interpretation of Conclusion:``\n",
        "\n",
        "Rejecting the null hypothesis implies that there are statistically significant differences between at least some of the group means. In other words, the evidence suggests that the observed differences in means are not due to random chance but likely due to an actual effect.\n",
        "\n",
        "However, the one-way ANOVA does not specify which specific groups have different means. To identify which groups are different from each other, you might need to perform post hoc tests (e.g., Tukey's HSD, Bonferroni correction) to conduct pairwise comparisons between groups.\n",
        "\n",
        "In summary, based on the given F-statistic of 5.23 and p-value of 0.02, you can conclude that there are statistically significant differences between the groups' means. The observed differences are unlikely to be due to random chance, suggesting that at least one group mean is significantly different from the others. Further analyses are needed to determine which specific groups exhibit these differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Handling missing data in a repeated measures ANOVA is important to ensure the accuracy and validity of your results. Different methods can be used to handle missing data, but the choice of method can impact the outcomes and interpretations. Here are some common methods and their potential consequences:\n",
        "\n",
        "1. **`Complete Case Analysis (Listwise Deletion)`**:\n",
        "   - **`Method`**: Exclude participants with missing data on any variable involved in the analysis.\n",
        "   - **`Consequences`**: This method can lead to a reduction in sample size and potential bias if missingness is related to the variables being analyzed. It might result in an unrepresentative sample, reduced statistical power, and generalizability issues.\n",
        "\n",
        "2. *`*Mean Imputation`**:\n",
        "   - **`Method`**: Replace missing values with the mean value of the available data for that variable.\n",
        "   - **`Consequences`**: This method can artificially reduce variability and may not accurately represent the actual data distribution. It can also lead to underestimation of standard errors and incorrect significance tests, as well as distort correlations and associations.\n",
        "\n",
        "3. **`Last Observation Carried Forward (LOCF)`**:\n",
        "   - **`Method`**: Replace missing values with the last observed value for that participant.\n",
        "   - **`Consequences`**: This method assumes that missing data do not change over time, which might not be true. It can lead to biased estimates, especially if the last observation was not typical. This method can also artificially strengthen the correlations between time points.\n",
        "\n",
        "4. **`Linear Interpolation`**:\n",
        "   - **`Method`**: Estimate missing values by interpolating between adjacent observed values.\n",
        "   - **`Consequences`**: Linear interpolation can provide better estimates than simple mean imputation but may not be suitable for nonlinear data patterns. It can distort the actual patterns and relationships in the data.\n",
        "\n",
        "5. **`Multiple Imputation`**:\n",
        "   - **`Method`**: Generate multiple plausible imputed datasets and analyze each separately, then combine the results.\n",
        "   - **`Consequences`**: Multiple imputation can provide more accurate and less biased estimates compared to other methods. However, it requires more complex analyses and software tools. It assumes that the missing data mechanism is Missing at Random (MAR), which might not hold in all cases.\n",
        "\n",
        "6. **`Model-Based Imputation`**:\n",
        "   - **`Method`**: Impute missing values using a statistical model fitted to the observed data.\n",
        "   - **`Consequences`**: This method can provide accurate estimates if the model fits well. However, if the model is misspecified, it can lead to incorrect imputations and biased results.\n",
        "\n",
        "The choice of method should depend on the nature of your data, the extent of missingness, and the assumptions you're willing to make about the missing data mechanism. Regardless of the method chosen, it's crucial to report how missing data were handled, assess the potential impact on the results, and consider sensitivity analyses to gauge the robustness of conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Post-hoc tests are conducted after a significant result in an analysis of variance (ANOVA) to further explore pairwise differences between specific groups. Since ANOVA only tells us that there are differences between groups, but not specifically which groups are different, post-hoc tests help us identify which groups' means are significantly different from each other. Here are some common post-hoc tests and when to use each one:\n",
        "\n",
        "1. **`Tukey's Honestly Significant Difference (HSD)`**:\n",
        "   - **`Usage`**: Tukey's HSD is used when you have three or more groups and you want to compare all possible pairs of group means to determine which pairs are significantly different.\n",
        "   - **`Example`**: In a study comparing the effectiveness of three different teaching methods, a significant ANOVA result suggests that there are differences between the methods. To identify which pairs of methods are different, you can use Tukey's HSD.\n",
        "\n",
        "2. **`Bonferroni Correction`**:\n",
        "   - **`Usage`**: The Bonferroni correction is used to control the familywise error rate when conducting multiple pairwise comparisons. It's more conservative and appropriate when you have many comparisons.\n",
        "   - **`Example`**: In genetics research, you are testing the effects of a treatment on the expression of multiple genes. You want to compare the treatment levels pairwise while controlling the overall error rate.\n",
        "\n",
        "3. **`Dunn's Test`**:\n",
        "   - **`Usage`**: Dunn's test is a nonparametric alternative to Tukey's HSD. It's used when the assumptions of homogeneity of variance and normality are violated.\n",
        "   - **`Example`**: In an ecological study, you're comparing the biodiversity of different habitats using a nonparametric ANOVA. You want to determine which habitat pairs have significantly different biodiversity levels.\n",
        "\n",
        "4. **`Scheffe's Test`**:\n",
        "   - **Usage**: Scheffe's test is used for situations where you need more conservative control over the familywise error rate. It's best when you're working with complex experimental designs.\n",
        "   - **`Example`**: In a medical trial with multiple treatment groups and placebo, you want to compare all possible combinations of treatments while accounting for the increased risk of type I error.\n",
        "\n",
        "5. **`Games-Howell Test`**:\n",
        "   - **`Usage`**: The Games-Howell test is used when the assumption of equal variances is not met (heteroscedasticity) and the sample sizes are unequal. It's a more robust alternative to Tukey's HSD.\n",
        "   - **`Example`**: You're studying the effects of different diets on weight loss, but the variances of weight loss in the diet groups are not equal, and sample sizes are different.\n",
        "\n",
        "`When to Use Post-Hoc Tests:`<br>\n",
        "Post-hoc tests should be used when you obtain a significant result in the ANOVA, indicating that there are overall differences between groups. You'd use them to identify which specific groups are different from each other. However, it's important to consider the potential risk of type I errors when conducting multiple tests. Choose a post-hoc test based on the assumptions of your data and your specific research question, and adjust for multiple comparisons if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Certainly, let's conduct a one-way ANOVA manually and interpret the results using the provided data.\n",
        "```python\n",
        "Given data:\n",
        "- Diet A: [1.5, 2.0, 1.8, 2.2, 1.7, ...] \n",
        "- Diet B: [1.2, 1.9, 1.5, 2.0, 1.6, ...]  \n",
        "- Diet C: [0.8, 1.0, 1.2, 0.9, 1.5, ...] \n",
        "\n",
        "`Step 1: Calculate Group Means`\n",
        "- Mean of Diet A = (1.5 + 2.0 + 1.8 + 2.2 + 1.7 + ...) / (number of observations)\n",
        "- Mean of Diet B = (1.2 + 1.9 + 1.5 + 2.0 + 1.6 + ...) / (number of observations)\n",
        "- Mean of Diet C = (0.8 + 1.0 + 1.2 + 0.9 + 1.5 + ...) / (number of observations)\n",
        "\n",
        "`Step 2: Calculate Total Sum of Squares (SST)`\n",
        "- SST = sum of squared differences between each data point and the overall mean\n",
        "\n",
        "`Step 3: Calculate Between-Group Sum of Squares (SSB)`\n",
        "- SSB = sum of (number of observations in each group) * (difference between group mean and overall mean) squared\n",
        "\n",
        "`Step 4: Calculate Within-Group Sum of Squares (SSW)`\n",
        "- SSW = sum of squared differences between each data point and its group mean\n",
        "\n",
        "`Step 5: Calculate the F-Statistic`\n",
        "- F = (SSB / (number of groups - 1)) / (SSW / (total number of observations - number of groups))\n",
        "\n",
        "`Step 6: Find the Critical F-Value`\n",
        "- Based on the significance level and degrees of freedom, find the critical F-value from an F-distribution table.\n",
        "\n",
        "`Step 7: Compare F-Statistic and Critical F-Value`\n",
        "- If the calculated F-Statistic is greater than the critical F-value, reject the null hypothesis. Otherwise, fail to reject the null hypothesis.\n",
        "\n",
        "`Step 8: Interpretation`\n",
        "- If the null hypothesis is rejected, it indicates that at least one group mean is significantly different from the others. If not rejected, there is insufficient evidence to conclude significant differences.\n",
        "\n",
        "Please note that manual calculations can be complex and time-consuming, especially if you have a large dataset. Using statistical software or libraries like Python's `scipy.stats` is recommended for practical analysis.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F-Statistic: 9.201581027667991\n",
            "P-Value: 0.003780723788810934\n",
            "There are significant differences between the mean weight loss of the diets.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Simulated data (replace with your actual data)\n",
        "diet_a = np.array([1.5, 2.0, 1.8, 2.2, 1.7])  # Diet A weight loss values\n",
        "diet_b = np.array([1.2, 1.9, 1.5, 2.0, 1.6])  # Diet B weight loss values\n",
        "diet_c = np.array([0.8, 1.0, 1.2, 0.9, 1.5])  # Diet C weight loss values\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(diet_a, diet_b, diet_c)\n",
        "\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"There are significant differences between the mean weight loss of the diets.\")\n",
        "else:\n",
        "    print(\"There are no significant differences between the mean weight loss of the diets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANOVA Table:\n",
            "                                sum_sq    df         F    PR(>F)\n",
            "C(Software)                 15.744596   2.0  2.120267  0.126377\n",
            "C(Experience)                0.545442   1.0  0.146905  0.702479\n",
            "C(Software):C(Experience)    1.451889   2.0  0.195521  0.822780\n",
            "Residual                   311.881986  84.0       NaN       NaN\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Simulated data\n",
        "software = np.repeat(['Program A', 'Program B', 'Program C'], 30)\n",
        "experience = np.tile(['Novice', 'Experienced'], 45)\n",
        "time_taken = np.random.normal(loc=10, scale=2, size=90)  # Generating random data\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({'Software': software, 'Experience': experience, 'TimeTaken': time_taken})\n",
        "\n",
        "# Fit the two-way ANOVA model\n",
        "model = ols('TimeTaken ~ C(Software) + C(Experience) + C(Software):C(Experience)', data=data).fit()\n",
        "\n",
        "# Perform the analysis of variance\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(\"ANOVA Table:\\n\", anova_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Two-Sample T-Test:\n",
            "T-Statistic: -2.710523708715754\n",
            "P-Value: 0.02663649662964512\n",
            "There is a significant difference in test scores between the two groups.\n",
            "Post-Hoc Test:\n",
            "   Multiple Comparison of Means - Tukey HSD, FWER=0.05    \n",
            "==========================================================\n",
            " group1    group2    meandiff p-adj  lower   upper  reject\n",
            "----------------------------------------------------------\n",
            "Control Experimental      6.0 0.0266 0.8954 11.1046   True\n",
            "----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Simulated data\n",
        "control_scores = np.array([75, 80, 85, 78, 82])  # Control group test scores\n",
        "experimental_scores = np.array([82, 88, 90, 84, 86])  # Experimental group test scores\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
        "\n",
        "print(\"Two-Sample T-Test:\")\n",
        "print(\"T-Statistic:\", t_statistic)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"There is a significant difference in test scores between the two groups.\")\n",
        "    # Perform post-hoc test\n",
        "    posthoc = pairwise_tukeyhsd(np.concatenate((control_scores, experimental_scores)),\n",
        "                                 np.concatenate((['Control'] * len(control_scores), ['Experimental'] * len(experimental_scores))))\n",
        "    print(\"Post-Hoc Test:\")\n",
        "    print(posthoc)\n",
        "else:\n",
        "    print(\"There is no significant difference in test scores between the two groups.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post- hoc test to determine which store(s) differ significantly from each other.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-Way ANOVA:\n",
            "F-Statistic: 344.1931736032215\n",
            "P-Value: 6.8727173149290494e-43\n",
            "There is a significant difference in daily sales between the three stores.\n",
            "Post-Hoc Test:\n",
            " Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
            "======================================================\n",
            "group1 group2 meandiff p-adj   lower    upper   reject\n",
            "------------------------------------------------------\n",
            "     A      B -22.0968 0.0226 -41.6285  -2.5651   True\n",
            "     A      C 174.1935    0.0 154.6618 193.7253   True\n",
            "     B      C 196.2903    0.0 176.7586  215.822   True\n",
            "------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Simulated data (replace with your actual data)\n",
        "store_a_sales = np.array([1000, 1100, 950, 1050, 1025, 980, 1010, 1035, 1005, 990, 1080, 1025, 1055, 950, 970, 1000, 1050, 990, 1020, 1035, 1075, 1025, 990, 1010, 1035, 1020, 1005, 980, 1050, 1025, 1030])  # Store A sales on each of the 30 days\n",
        "store_b_sales = np.array([900, 950, 1000, 1050, 980, 970, 1020, 980, 1005, 990, 1020, 1050, 1055, 960, 990, 975, 1000, 1025, 990, 980, 1030, 1005, 990, 1000, 980, 1025, 980, 1000, 1020, 970, 990])   # Store B sales on each of the 30 days\n",
        "store_c_sales = np.array([1200, 1250, 1150, 1100, 1180, 1220, 1190, 1230, 1205, 1160, 1230, 1150, 1165, 1190, 1210, 1175, 1190, 1215, 1180, 1200, 1210, 1175, 1195, 1180, 1200, 1225, 1210, 1195, 1165, 1200, 1220]) # Store C sales on each of the 30 days\n",
        "\n",
        "# Stack data into one array\n",
        "all_sales = np.concatenate((store_a_sales, store_b_sales, store_c_sales))\n",
        "\n",
        "# Create group labels\n",
        "groups = np.array(['A'] * len(store_a_sales) + ['B'] * len(store_b_sales) + ['C'] * len(store_c_sales))\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(store_a_sales, store_b_sales, store_c_sales)\n",
        "\n",
        "print(\"One-Way ANOVA:\")\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"There is a significant difference in daily sales between the three stores.\")\n",
        "    # Perform post-hoc test\n",
        "    posthoc = pairwise_tukeyhsd(all_sales, groups)\n",
        "    print(\"Post-Hoc Test:\")\n",
        "    print(posthoc)\n",
        "else:\n",
        "    print(\"There is no significant difference in daily sales between the three stores.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOINuiGOcJK4"
      },
      "source": [
        "<h1 style = 'color:orange'>\n",
        "    <b><div>üôèüôèüôèüôèüôè       THANK YOU        üôèüôèüôèüôèüôè</div></b>\n",
        "</h1>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
