{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHKORN_4cJKm"
      },
      "source": [
        "<h1 style = 'color:red'><b>Week-11, Statistics Advance-2 Assignment</b><h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUW29xlcJKo"
      },
      "source": [
        "Name - Gorachanda Dash <br>\n",
        "Date - 09-Mar-2023<br>\n",
        "Week 11, Statistics Advance-2 Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKnfvSL5cJK2"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example.</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Probability Mass Function (PMF)` and `Probability Density Function (PDF)` are statistical functions used to describe the probability distribution of a discrete random variable and a continuous random variable, respectively.\n",
        "\n",
        "1. `Probability Mass Function (PMF):`\n",
        "The PMF is a function that provides the probability of a discrete random variable taking on a specific value. It gives the probability for each possible outcome of the random variable. The sum of all probabilities in the PMF is equal to 1.\n",
        "\n",
        "Example of PMF:\n",
        "Let's consider a fair six-sided die. The PMF for this die would be:\n",
        "PMF(x) = 1/6 for x = 1, 2, 3, 4, 5, 6\n",
        "PMF(x) = 0 for all other values of x\n",
        "\n",
        "Here, PMF(1) = 1/6 means that the probability of rolling a 1 on the die is 1/6.\n",
        "\n",
        "2. `Probability Density Function (PDF):`\n",
        "The PDF is a function that represents the probability distribution of a continuous random variable. Unlike PMF, which deals with discrete values, PDF deals with continuous values. The area under the PDF curve over a given interval represents the probability of the random variable falling within that interval.\n",
        "\n",
        "Example of PDF:\n",
        "Consider a normally distributed continuous random variable with a mean (μ) of 0 and a standard deviation (σ) of 1. The PDF for this normal distribution is given by:\n",
        "PDF(x) = (1 / (√(2π) * σ)) * e^((-1/2) * ((x - μ) / σ)^2)\n",
        "\n",
        "The above PDF equation represents a standard normal distribution. For any specific value of x, the PDF value gives the probability density at that point. However, since the normal distribution is continuous, the probability of obtaining any specific value is infinitesimally small. Instead, we use the area under the PDF curve to find probabilities for specific intervals.\n",
        "\n",
        "For example, to find the probability that the random variable falls between -1 and 1, we integrate the PDF over that interval:\n",
        "Probability (-1 ≤ x ≤ 1) = ∫[from -1 to 1] (1 / (√(2π) * 1)) * e^((-1/2) * ((x - 0) / 1)^2) dx\n",
        "\n",
        "The value obtained from this integration gives the probability of the random variable falling between -1 and 1 in a standard normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nokEV7cJK3"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Cumulative Density Function (CDF) is a statistical function used to describe the cumulative probability distribution of a random variable. It gives the probability that the random variable takes on a value less than or equal to a given value. In other words, it represents the accumulation of probabilities as we move along the range of the random variable.\n",
        "\n",
        "Example of CDF:<br>\n",
        "Let's consider a fair six-sided die. The CDF for this die would be:<br>\n",
        "CDF(x) = 0 for x < 1<br>\n",
        "CDF(x) = 1/6 for 1 ≤ x < 2<br>\n",
        "CDF(x) = 1/3 for 2 ≤ x < 3<br>\n",
        "CDF(x) = 1/2 for 3 ≤ x < 4<br>\n",
        "CDF(x) = 2/3 for 4 ≤ x < 5<br>\n",
        "CDF(x) = 5/6 for 5 ≤ x < 6<br>\n",
        "CDF(x) = 1 for x ≥ 6\n",
        "\n",
        "Here, CDF(3) = 1/2 means that the probability of rolling a value less than or equal to 3 on the die is 1/2.\n",
        "\n",
        "`Why CDF is used?`<br>\n",
        "The CDF is useful in various statistical and probability calculations because it provides important information about the distribution of a random variable. Some key advantages of using the CDF are:\n",
        "\n",
        "1. `Probability Calculation:` The CDF allows us to find the probability of the random variable falling within a specific range. This is especially useful for calculating probabilities for continuous random variables.\n",
        "\n",
        "2. `Percentiles and Quartiles:` The CDF can be used to find percentiles and quartiles of a distribution, which helps in understanding the spread of the data and identifying key values.\n",
        "\n",
        "3. `Comparing Distributions:` The CDF enables us to compare different distributions and assess how they differ in terms of their probabilities and spread.\n",
        "\n",
        "4. `Reliability and Survival Functions:` In survival analysis, the CDF is used to calculate the reliability (probability of survival) and survival (probability of failure) functions.\n",
        "\n",
        "Overall, the CDF provides a comprehensive view of the probabilities associated with a random variable and is a fundamental tool in probability theory and statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMuh9StycJK3"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q3: What are some examples of situations where the normal distribution might be used as a model?<br>\n",
        "Explain how the parameters of the normal distribution relate to the shape of the distribution.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The normal distribution, also known as the Gaussian distribution, is widely used as a model in various real-world situations due to its many desirable properties. Some examples of situations where the normal distribution might be used as a model include:\n",
        "\n",
        "1. `Heights and Weights:` Human heights and weights often follow a roughly normal distribution. For example, if we collect data on the heights of adult males in a population, the distribution is likely to be approximately normal.\n",
        "\n",
        "2. `Test Scores:` In educational testing, scores on standardized tests such as the SAT or IQ tests are often modeled using a normal distribution.\n",
        "\n",
        "3. `Errors in Measurements:` In many scientific experiments and measurements, the errors or residuals are assumed to be normally distributed.\n",
        "\n",
        "4. `Financial Returns:` In finance, daily stock returns or investment returns are often assumed to follow a normal distribution.\n",
        "\n",
        "5. `Natural Phenomena:` Many natural phenomena, such as the distribution of rainfall or temperatures, can be approximated by a normal distribution.\n",
        "\n",
        "`Parameters of the Normal Distribution:`<br>\n",
        "The normal distribution is characterized by two parameters: the mean (μ) and the standard deviation (σ). These parameters determine the shape and location of the distribution:\n",
        "\n",
        "1. `Mean (μ):` The mean represents the center of the distribution and is also the expected value of the random variable. It determines the location of the peak or the center of symmetry.\n",
        "\n",
        "2. `Standard Deviation (σ):` The standard deviation measures the spread or dispersion of the data points around the mean. A larger standard deviation means the data points are more spread out, while a smaller standard deviation indicates that the data points are closer to the mean.\n",
        "\n",
        "The shape of the normal distribution is symmetric around the mean, and approximately 68% of the data falls within one standard deviation of the mean (between μ - σ and μ + σ). About 95% of the data falls within two standard deviations of the mean (between μ - 2σ and μ + 2σ), and about 99.7% of the data falls within three standard deviations of the mean (between μ - 3σ and μ + 3σ). This characteristic of the normal distribution is known as the 68-95-99.7 rule or the empirical rule.\n",
        "\n",
        "In summary, the mean and standard deviation of the normal distribution play a critical role in shaping the distribution and determining how the data points are distributed around the center."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aKHN4uEcJK4"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The normal distribution is of great importance in statistics and data analysis due to its numerous applications and several desirable properties. Some of the key reasons why the normal distribution is important are:\n",
        "\n",
        "1. `Central Limit Theorem:` One of the fundamental concepts in statistics, the Central Limit Theorem, states that the sum or average of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the original distribution. This property allows statisticians to make inferences about population parameters based on sample statistics.\n",
        "\n",
        "2. `Easy Analysis:` The normal distribution is mathematically well-behaved, making it easier to perform analytical and statistical calculations. It allows for the use of various mathematical tools, such as the Z-score, to analyze data.\n",
        "\n",
        "3. `Parameter Estimation:` Many statistical methods, such as maximum likelihood estimation, assume that the data is normally distributed. This makes parameter estimation and hypothesis testing more straightforward.\n",
        "\n",
        "4. `Prediction and Forecasting:` In many real-life situations, future values can be predicted using normal distribution-based models. For example, in finance, stock returns or asset prices are often modeled using a normal distribution to forecast future values.\n",
        "\n",
        "5. `Standardization:` The Z-score, which is calculated using the normal distribution, allows us to standardize data and compare it across different scales or units.\n",
        "\n",
        "`Examples of Real-Life Normal Distributions:`\n",
        "\n",
        "1. `Heights of Adult Males:` The distribution of heights of adult males in a population is often modeled by a normal distribution.\n",
        "\n",
        "2. `IQ Scores:` Intelligence quotient (IQ) scores of a large population tend to follow a normal distribution.\n",
        "\n",
        "3. `Errors in Measurements:` In many scientific experiments and measurements, the errors or residuals are assumed to be normally distributed.\n",
        "\n",
        "4. `Exam Scores:` Scores on standardized tests such as the SAT or GRE often follow a normal distribution.\n",
        "\n",
        "5. `Daily Temperature:` The distribution of daily temperatures in a particular region over a long period of time is often approximately normal.\n",
        "\n",
        "6. `Weight of Newborns:` The weight of newborn babies in a hospital is often modeled using a normal distribution.\n",
        "\n",
        "These are just a few examples of situations where the normal distribution is commonly used as a model. In many cases, while the actual data may not perfectly follow a normal distribution, the normal distribution provides a good approximation, and its properties make it a valuable tool in statistical analysis and modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2avHLHDcJK4"
      },
      "source": [
        "<p style=\" color : #4233FF\"><b>Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Bernoulli distribution` is a discrete probability distribution that describes a random experiment with two possible outcomes: success (usually represented by 1) and failure (usually represented by 0). It is named after the Swiss mathematician Jacob Bernoulli. The distribution is characterized by a single parameter, p, which represents the probability of success in a single trial.\n",
        "\n",
        "`Example of Bernoulli Distribution:`\n",
        "An example of a Bernoulli distribution is flipping a fair coin. Let's consider \"success\" as getting a head and \"failure\" as getting a tail. The probability of getting a head (success) in a single coin flip is 0.5, and the probability of getting a tail (failure) is also 0.5. Thus, this can be modeled using a Bernoulli distribution with p = 0.5.\n",
        "\n",
        "Difference between Bernoulli Distribution and Binomial Distribution:<br>\n",
        "1. `Number of Trials:`\n",
        "   - Bernoulli Distribution: Represents a single trial or one-time experiment with two possible outcomes.\n",
        "   - Binomial Distribution: Represents the number of successes in a fixed number of independent Bernoulli trials.\n",
        "\n",
        "2. `Parameters:`\n",
        "   - Bernoulli Distribution: Has a single parameter p, which is the probability of success in a single trial.\n",
        "   - Binomial Distribution: Has two parameters, n and p, where n is the number of trials and p is the probability of success in each trial.\n",
        "\n",
        "3. `Outcomes:`\n",
        "   - Bernoulli Distribution: Has only two possible outcomes, 0 and 1, corresponding to failure and success, respectively.\n",
        "   - Binomial Distribution: Represents the number of successes (k) in n independent trials, and the possible outcomes are 0, 1, 2, ..., n.\n",
        "\n",
        "4. `Probability Mass Function (PMF):`\n",
        "   - Bernoulli Distribution: PMF is given by P(X = x) = p^x * (1-p)^(1-x), where x is either 0 or 1.\n",
        "   - Binomial Distribution: PMF is given by P(X = k) = C(n, k) * p^k * (1-p)^(n-k), where k is the number of successes.\n",
        "\n",
        "In summary, the Bernoulli distribution is used to model a single binary event with two possible outcomes, while the binomial distribution is used to model the number of successes in a fixed number of independent Bernoulli trials. The binomial distribution is a generalization of the Bernoulli distribution when dealing with multiple trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we can use the z-score formula and standard normal distribution table.\n",
        "\n",
        "The z-score formula is given by:\n",
        "`z = (X - μ) / σ`\n",
        "\n",
        "Where:<br>\n",
        "X is the value we want to find the probability for (in this case, X = 60),<br>\n",
        "μ is the mean of the dataset (μ = 50), and<br>\n",
        "σ is the standard deviation of the dataset (σ = 10).<br>\n",
        "\n",
        "Now, we can calculate the z-score:\n",
        "z = (60 - 50) / 10 = `1`\n",
        "\n",
        "We can use a standard normal distribution table or a statistical software/tool to find this value. For a `z-score of 1`, the cumulative probability is approximately `0.8413.`\n",
        "\n",
        "Finally, the probability that a randomly selected observation will be greater than 60 is given by:\n",
        "`P(X > 60) = 1 - P(X ≤ 60) = 1 - 0.8413 ≈ 0.1587`\n",
        "\n",
        "So, the probability is approximately 0.1587, which means there is about a 15.87% chance that a randomly selected observation will be greater than 60 in the given normally distributed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability (X > 60): 0.15865525393145707\n"
          ]
        }
      ],
      "source": [
        "# Alternative\n",
        "import scipy.stats as st\n",
        "\n",
        "# Define the parameters of the normal distribution\n",
        "mean = 50\n",
        "std_dev = 10\n",
        "\n",
        "# Calculate the z-score for X = 60\n",
        "X = 60\n",
        "z_score = (X - mean) / std_dev\n",
        "\n",
        "# Calculate the cumulative probability using the cumulative distribution function (CDF)\n",
        "probability = 1 - st.norm.cdf(z_score)\n",
        "\n",
        "print(\"Probability (X > 60):\", probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q7: Explain uniform Distribution with an example.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Uniform distribution` is a probability distribution where all outcomes within a specific range are equally likely to occur. In other words, each value within the range has the same probability of being observed. The probability density function (PDF) of a continuous uniform distribution is constant over the given range and zero outside that range.\n",
        "\n",
        "`Example:`<br>\n",
        "Let's consider a fair six-sided die. When you roll the die, each face (1, 2, 3, 4, 5, or 6) has an equal probability of appearing. The probability of rolling any particular number is 1/6 since there are six equally likely outcomes.\n",
        "\n",
        "For a continuous uniform distribution, consider a random number generator that produces a random value between 0 and 1 (inclusive) with equal probability for any value within that range. Any number between 0 and 1 has an equal chance of being generated.\n",
        "\n",
        "In both cases, the outcomes are equally distributed, and there is no preference for any particular value within the range. This uniformity is the defining characteristic of the uniform distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q8: What is the z score? State the importance of the z score.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The z-score, also known as the standard score, is a statistical measurement that describes a value's relationship to the mean of a group of values and measures how many standard deviations it is above or below the mean. The z-score is calculated by subtracting the mean from the individual data point and then dividing the result by the standard deviation.\n",
        "\n",
        "The formula for calculating the z-score of a data point x in a dataset with mean μ and standard deviation σ is:\n",
        "\n",
        "`z = (x - μ) / σ`\n",
        "\n",
        "The importance of the z-score lies in its ability to standardize different datasets and allow for meaningful comparisons. Some key points about the z-score's importance:\n",
        "\n",
        "1. `Standardization:` Z-scores standardize data, making it easier to compare values from different datasets that have different scales and units. It transforms data to a common scale with a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "2. `Outlier Detection:` Z-scores can be used to identify outliers in a dataset. Data points with z-scores greater than a certain threshold (e.g., |z| > 2 or |z| > 3) are considered outliers, indicating that they deviate significantly from the mean.\n",
        "\n",
        "3. `Probability Estimation:` Z-scores are used in normal distribution tables to estimate probabilities. By converting raw data into z-scores, one can find the probability of a value occurring within a certain range of standard deviations from the mean.\n",
        "\n",
        "4. `Hypothesis Testing:` Z-tests and Z-confidence intervals use z-scores to compare sample means with known population parameters or to make inferences about a population mean.\n",
        "\n",
        "Overall, the z-score is a powerful statistical tool that helps in understanding the relative position of data points within a distribution and enables more meaningful analysis and comparisons across different datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.?</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the distribution of the sample mean of a large number of independent and identically distributed (i.i.d.) random variables will be approximately normally distributed, regardless of the shape of the original population distribution. This holds true, especially when the sample size is sufficiently large (typically n ≥ 30).\n",
        "\n",
        "The significance of the Central Limit Theorem lies in its practical applications and implications:\n",
        "\n",
        "1. `Normal Approximation:` The CLT allows us to approximate the distribution of the sample mean, even if the underlying population is not normally distributed. This is crucial in various statistical methods and hypothesis testing, where assumptions of normality are often required.\n",
        "\n",
        "2. `Estimation`: The CLT enables us to estimate population parameters based on sample statistics. For instance, it allows us to construct confidence intervals for the population mean, which helps in making inferences about the entire population.\n",
        "\n",
        "3. `Inferential Statistics:` The CLT forms the basis of many inferential statistical techniques, such as z-tests and t-tests, which rely on the assumption of normality for their validity.\n",
        "\n",
        "4. `Large Sample Sizes:` The CLT implies that as the sample size increases, the sample mean will more closely follow a normal distribution. This is useful in situations where collecting a large sample is feasible and results in more reliable statistical analysis.\n",
        "\n",
        "5. `Real-World Data:` Many natural phenomena and social processes can be approximated as the sum of many small, independent effects. The CLT allows us to understand and analyze such complex systems using simple and powerful tools based on the normal distribution.\n",
        "\n",
        "In summary, the Central Limit Theorem is a cornerstone of statistics, providing a theoretical foundation for various statistical methods and allowing us to make inferences about a population based on a sample. It plays a crucial role in practical data analysis, as it enables us to work with normally distributed sample means, even when the underlying population distribution might not be normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\" color : #4233FF\"><b>Q10: State the assumptions of the Central Limit Theorem.</b>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Central Limit Theorem (CLT) relies on the following assumptions:\n",
        "\n",
        "1. `Random Sampling:` The samples used to calculate the sample mean or sum are chosen randomly from the population of interest. Each sample should be independent of the others.\n",
        "\n",
        "2. `Identically Distributed:` All samples are drawn from the same population and follow the same probability distribution, regardless of the population's distribution shape.\n",
        "\n",
        "3. `Finite Variance:` The population from which the samples are drawn has a finite variance (i.e., the variance of the population is not infinite).\n",
        "\n",
        "4. `Sample Size:` The sample size should be sufficiently large. While there is no strict rule on what constitutes a \"sufficiently large\" sample size, a common rule of thumb is n ≥ 30. However, for populations with highly skewed distributions or heavy tails, a larger sample size may be required.\n",
        "\n",
        "It's important to note that the Central Limit Theorem is applicable to various distributions, not just normal distributions. As long as the assumptions mentioned above are met, the sample mean will tend to follow a normal distribution with the population mean as its center and a standard deviation related to the population standard deviation and sample size.\n",
        "\n",
        "The CLT is one of the fundamental concepts in statistics and has wide applications in hypothesis testing, confidence intervals, and other statistical analyses. However, researchers should be mindful of the assumptions and validity of using the CLT in specific situations and consider alternative methods if these assumptions are not met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOINuiGOcJK4"
      },
      "source": [
        "<h1 style = 'color:orange'>\n",
        "    <b><div>THANK YOU</div></b>\n",
        "</h1>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
